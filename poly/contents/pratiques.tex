%-*- coding: utf-8 -*-
\label{chap:pratiques}

\paragraph{Notions :} visualisation de données, représentativité des données,
équité des algorithmes, confidentialité des données, anonymisation,
responsabilité.

\paragraph{Objectifs pédagogiques :} 
\begin{itemize}      
  \setlength{\itemsep}{3pt}
\item S'interroger sur la pertinence d'une analyse de données et la validité
  des conclusions qui en sont tirées.
\end{itemize}

La science des données n'est pas uniquement une discipline technique : comme
souvent en ingénierie, nous ne pouvons pas dissocier les calculs que nous
faisons de la question posée ni de leur utilisation. Ce chapitre n'a pas
vocation à être un cours d'éthique\footnote{L'éthique peut être définie comme
  l'étude de la justification d'ue action à partir de normes, règles juridiques
  ou déontologiques, valeurs morales, intuitions et traditions qui peuvent être
  multiples et contradictoires au sein d'une même société.}, mais à vous donner
quelques points d'entrée pour vous amener à vous poser des questions sur
l'usage de la science des données, de l'apprentissage automatique et de
l'intelligence artificielle. Pour cette raison, vous trouverez plus de liens
externes (cliquables dans la version PDF de ce document) qu'à l'habitude à
travers le texte de ce chapitre, pointant tant vers des publications
scientifiques que des blogs de vulgarisation ou des articles de presse grand
public. N'hésitez pas à poursuivre vos propres lectures sur le sujet.

Nous motiverons ce chapitre par deux citations : la première, attribuée à
Benjamin Disraeli par Mark Twain, ``\textit{There are three kinds of lies:
  lies, damned lies, and statistics}'', et la seconde, attribuée à George
Box, ``\textit{All models are wrong, but some are useful}''.

\section{Visualisation de données}
La façon dont vous choisissez de représenter vos données ou vos résultats a un
impact fort sur le message que vous essayez de faire passer. 

Mi-mai 2020, le Department of Public Health de l'État de Géorgie (États-Unis
d'Amérique) a publié le diagramme en barres de la
figure~\ref{fig:georgia_wtf_barplot}. Regardez bien l'axe des abscisses : le
message vous semble-t-il le même quand les dates sont ordonnées de manière
chronologique, comme sur la figure~\ref{fig:georgia_fixed_barplot} ?

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pratiques/georgia_wtf_barplot}
    \caption{Première version du diagramme en barres.}
    \label{fig:georgia_wtf_barplot}
  \end{subfigure} \hfill
  \begin{subfigure}[t]{0.47\textwidth}
    \includegraphics[width=\textwidth]{figures/pratiques/georgia_fixed_barplot}  
    \caption{Deuxième version du diagramme en barres.}
    \label{fig:georgia_fixed_barplot}
  \end{subfigure}  
  \caption{Deux variantes du même diagramme en barres publiées par le
    Department of Public Health de l'État de Géorgie à propos du nombre de cas
    de CoVid19.}
  %\label{fig:georgia_barplot}
\end{figure}

Il est donc très important de vous assurez que vos graphiques soient lisibles
et qu'ils traduisent clairement votre message sans déformer les données. La
visualisation des données, ou \textit{dataviz}, est un champ d'études à part
entière.  Nous nous contenterons ici de citer quelques principes parmi les plus
importants.

\subsection{Des graphiques clairs et lisibles}
Un bon graphique doit pouvoir être compréhensible de manière autonome,
c'est-à-dire sans référence au texte. Pour cela, quelques éléments généraux,
valables bien au-delà de ce cours :
\begin{itemize}
\item Pour être compréhensible, un graphique doit comporter un certain nombre
  d'éléments indispensables à sa compréhension, et en particuilier :
  \begin{itemize}
  \item un titre ;
  \item une légende ;
  \item le nom des axes, l'unité des variables représentées, et l'échelle si
    elle n'est pas linéaire (par exemple, échelle logarithmique).
  \end{itemize}
\item Pour qu'un graphique soit lisible, ses éléments doivent être suffisamment
  grands. Attention en particulier à :
  \begin{itemize}
  \item la taille des textes (légendes, graduations, etc.) ;
  \item la taille des marqueurs et l'épaisseur des traits.
  \end{itemize}
\item Pour être lisible, un graphique ne doit pas comporter d'éléments
  superflus. En particulier, il vaut mieux éviter
  \begin{itemize}
  \item de représenter trop d'informations/éléments à la fois ;
    il est difficile de garder en mémoire plus de 7--10 éléments à la fois
  \item d'utiliser trop de couleurs différentes, surtout si elles ne
    contiennent pas d'information.
  \end{itemize}
\end{itemize}

\subsection{Le choix des axes}
%https://callingbullshit.org/tools/tools_misleading_axes.html 
Le choix des échelles et intervalles d'un graphique a une influence sur son
interprétation.

Pour un diagramme en barres, ne pas faire commencer les axes à 0 peut
artificiellement gonfler les différences entre les différentes barres. Ainsi,
le diagramme de la figure~\ref{fig:bars_start_nonzero} indique que le modèle 4
est bien supérieur aux autres, tandis que celui de la
figure~\ref{fig:bars_start_zero} montre des performances très comparables entre
les différentes méthodes. (Dans ce cas précis, il serait de toute façon
souhaitable de répéter plusieurs fois l'entrainement et l'évaluation, par
exemple avec une validation croisée (que nous verrons
section~\ref{sec:crossval}) et de produire des barres d'erreurs.)
\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pratiques/bars_start_nonzero}
    \caption{Axe des ordonnées réduit.}
    \label{fig:bars_start_nonzero}
  \end{subfigure} \hfill
  \begin{subfigure}[t]{0.47\textwidth}
    \includegraphics[width=\textwidth]{figures/pratiques/bars_start_zero}  
    \caption{Axe des ordonnées allant de 0 à 1.}
    \label{fig:bars_start_zero}
  \end{subfigure}  
  \caption{Deux façons de présenter la comparaison des performances de 4 modèles.}
  %\label{fig:georgia_barplot}
\end{figure}

À l'inverse, il pourra être préférable pour un diagramme dont le but est non
pas de comparer les valeurs absolues de variables mais plutôt de présenter leur
évolution que l'axe des ordonnées ne commence pas à zéro. Ainsi, la
figure~\ref{fig:line_start_zero} indique une température très stable, tandis
que la figure~\ref{fig:line_start_nonzero} permet de mieux rendre compte des
variations.
\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pratiques/line_start_zero}
    \caption{Axe des ordonnées partant de 0K.}
    \label{fig:line_start_zero}
  \end{subfigure} \hfill
  \begin{subfigure}[t]{0.47\textwidth}
    \includegraphics[width=\textwidth]{figures/pratiques/line_start_nonzero}  
    \caption{Axe des ordonnées réduit.}
    \label{fig:line_start_nonzero}
  \end{subfigure}  
  \caption{Deux façons de présenter l'évolution des températures moyenne de la
    table~\ref{tab:meteo_data}.}
  %\label{fig:georgia_barplot}
\end{figure}

\subsection{\textit{Proportional ink} ou principe de l'encre proportionnelle}
%https://callingbullshit.org/tools/tools_proportional_ink.html
De manière générale, il est recommandé, lorsque l'on utilise des surfaces pour
représenter des nombres (par exemple, les rectangles d'un diagramme en barres),
que ces surfaces soient d'aires proportionnelles aux nombres en question. On
retrouve d'ailleurs ici l'idée de commencer les barres d'un diagramme en
barres à 0.

Il faut cependant faire aussi attention à ce que les surfaces en question
soient faciles à comparer visuellement. Un diagramme camembert est ainsi
préférable à un graphique à bulles ; mais un diagramme en barres est
généralement plus lisible qu'un diagramme camembert. La figure~\ref{fig:areas}
l'illustre. Il s'agit d'une variante d'une
\href{https://www.jstor.org/stable/2288400}{expérience
  menée au début des années 1980} et souvent considérée comme fondatrice en
\textit{dataviz}.

Remarquez ici que le diagramme en barres serait encore plus lisible sans couleurs (elles n'apportent rien) et en ordonnant les catégories par proportion.
\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.20\textwidth}
    \includegraphics[width=\textwidth]{figures/pratiques/areas_bubbles}  
    \caption{Graphique à bulles.}
    \label{fig:areas_bubbles}
  \end{subfigure}  \hfill
  \begin{subfigure}[t]{0.33\textwidth}
    \includegraphics[width=\textwidth]{figures/pratiques/areas_pie}  
    \caption{Diagramme camembert.}
    \label{fig:areas_pie}
  \end{subfigure} \hfill
  \begin{subfigure}[t]{0.33\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pratiques/areas_bars}
    \caption{Diagramme en barres.}
    \label{fig:areas_bars}
  \end{subfigure} 
  \caption{Trois façons de représenter les proportions de 8 catégories. Quelle(s) représentation(s) permettent de les classer aisément par ordre croissant ?}
  \label{fig:areas}
\end{figure}

%\subsection{Attention aux résumés}
%https://python-graph-gallery.com/39-hidden-data-under-boxplot/ 


\subsection{Dyschromatopie}
Nous ne percevons pas les couleurs de la même façon. Une forte proportion de la population est atteinte d'une forme ou d'une autre de dyschromatopie, la plus fréquente étant la deutéranopie (incapacité de différencier rouge et vert). 

Pour assurer une accessibilité maximale, utilisez des échelles de couleurs
adaptées. Il est difficile de s'adapter à \textit{toutes} les dyschromatopies ;
néanmoins le cycle par défaut de \texttt{matplotlib} est supposé être
relativement adapté. Pour des \textit{heatmaps}, favoriser les échelles de
couleur \textit{viridis} ou \textit{cividis} (voir
figure~\ref{fig:pca_plot}). Des outils comme \href{https://www.color-blindness.com/coblis-color-blindness-simulator/}{CBLIS} ou \href{https://www.funkify.org}{Funkify} vous permettent de simuler différentes dyschromatopies pour vérifier la lisibilité de vos graphiques.

Vous pouvez aussi augmenter la lisibilité de vos graphiques en utilisant des
indices supplémentaires (épaisseur de trait, hachures, forme des points,
ordonner les légendes dans le même ordre que les courbes, etc.) et en doublant
vos images d'une description textuelle alternative pour les personnes
non-voyantes.


\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.30\textwidth}
    \includegraphics[width=\textwidth]{figures/pratiques/pca_plot_magma}  
    \caption{Magma.}
    \label{fig:pca_plot_magma}
  \end{subfigure}  \hfill
  \begin{subfigure}[t]{0.30\textwidth}
    \includegraphics[width=\textwidth]{figures/pratiques/pca_plot_viridis}  
    \caption{Viridis.}
    \label{fig:pca_plot_viridis}
  \end{subfigure} \hfill
  \begin{subfigure}[t]{0.30\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pratiques/pca_plot_cividis}
    \caption{Cividis.}
    \label{fig:pca_plot_cividis}
  \end{subfigure} 
  \caption{Athlètes de la PC2, représentés selon deux composantes, et colorés
    en fonction de leur classement, selon trois échelles de couleur
    différentes.}
  \label{fig:pca_plot}
\end{figure}

\section{Équité des algorithmes}
Une question importante qui se pose constamment en science des données est
celle de la \textbf{reproduction des biais}. En effet, un modèle appris sur un
jeu de données peut facilement reproduire des biais de ce jeu de données,
qu'ils soient explicites ou implicites.

Un exemple qui revient souvent est celui d'un algorithme de ressources humaines
utilisé par Amazon. Le modèle avait tendance à rejeter les candidatures posées
par des femmes. En effet, il était entraîné sur des données internes à
l'entreprise, dont les recrutements étaient fortement biaisés en faveur des
hommes. Bien que le genre n'ait pas été une variable utilisée pour décrire les
candidatures, le modèle détectait dans le texte des CV des informations
corrélée dans le jeu d'entraînement au rejet d'une candidature mais qui
s'avéraient surtout traduire qu'elle était posée par une femme (éducation dans
un établissement non-mixte réservé aux femmes ; appartenance à une équipe de
sport féminin, etc.).

Ainsi, ce n'est pas parce qu'un modèle statistique est purement mathématique
qu'il est impartial ; en particulier, un modèle ne peut pas être de meilleure
qualité que son jeu d'entraînement. Il faut donc réfléchir à la
\textbf{représentativité} des données : peut-on bien considérer qu'il s'agit
d'un échantillon aléatoire de la population qui nous intéresse, où ne
correspondent-elles qu'à une sous-population spécifique ?

Un autre exemple de reproduction des biais apparait dans une publication de
2016 qui présente un classifieur capable de distinguer criminels de
non-criminels à partir de simples photos. Cependant, les clichés de criminels
étaient des photos administratives prises de face, sans sourire, tandis que les
photos de non-criminels étaient des clichés plus flatteurs : le modèle
 \href{https://callingbullshit.org/case\_studies/case\_study\_criminal\_machine\_learning.html}{détectait
  en fait les sourires}. On retrouve très souvent ce type d'erreurs, dûes à un
\textbf{facteur confondant} : on croit arriver à séparer des images sur leur
contenu alors qu'on utilise principalement leur luminosité ; ou à trouver des
facteurs génétiques influençant le niveau économique, alors que celui-ci est
fortement corrélé dans les données à la couleur de peau ; et ainsi de suite.

Cette étude soulève par ailleurs une question plus large, qui est celle de la
pertinence de ce genre de modèle ; la question de l'équité des algorithmes ne
se ramène pas qu'aux biais dans les données, mais peut aussi concerner leur
pré-traitement, le choix de la question qu'on leur fait résoudre, ou les
décisions prises sur leurs résultats.

La question de l'équité des algorithmes est un sous-domaine important de
l'apprentissage automatique, et se pose d'autant plus que ses applications
s'étendent à des domaines divers et variés touchant de nombreux aspects de nos
sociétés : recrutement mais aussi sécurité, santé, justice, etc. 
C'est le sujet par exemple de l'organisation
\href{https://facctconference.org}{Fairness, Accountability and Transparency in
  Machine Learning}.

Pour autant, il n'y a pas actuellement (et il n'y aura vraisemblablement
jamais) d'outils ou de procédures permettant de garantir cette équité. Il est
ainsi nécessaire de comprendre l'origine possible des biais, ainsi que de
développer des outils pour les mesurer.

Si quelques outils pour l'évaluation d'outils numériques du point de vue
éthique ont vu le jour ces dernières années, comme
\href{http://aequitas.dssg.io/}{Aequitas} aux USA, ceux présentés dans une précédente version de ce poly ont déjà disparu, illustrant les difficultés de ce sujet.

\section{Fiabilité}
Du diagnostic automatisé aux véhicules autonomes, nous avons de plus en plus
envie d'utiliser l'intelligence artificielle, qui présente de nombreuses
opportunités. Mais comment faire confiance aux modèles et algorithmes qui en
sont issus ? Plusieurs questions se posent en plus de celle de l'équité
discutée plus haut.

\paragraph{Vérifiabilité} les systèmes d'IA ont-ils le comportement attendu ?
Les méthodes formelles typiquement utilisées en informatique pour les
programmes utilisés en avionique ne se prêtent guère aux modèles de
l'apprentissage automatique, même si \href{https://formal-paris-saclay.fr/}{de
récents travaux émergent sur le sujet}.

\paragraph{Explicabilité et interprétabilité} Il s'agit aussi de vastes champs
d'étude. Si une régression linéaire est relativement interprétable (cf. PC 3),
des modèles paramétriques plus complexes tels que ceux produits par des réseaux
de neurones artificiels (voir chapitre~\ref{chap:nonlin}) le
sont beaucoup moins. 

\paragraph{Spécification} La description précise du comportement attendu
peut-elle aussi être délicate : quel choix doit faire un véhicule autonome
entre renverser une fillette et emboutir une moto avec deux passagers ? Le MIT
Media Lab propose par exemple \href{http://moralmachine.mit.edu/hl/fr}{La
  Machine Morale}, une plateforme permettant d'explorer divers dilemmes moraux
posés par la prise de décision de machines intelligentes.

\paragraph{Robustesse} Les modèles sont-ils robustes aux attaques ? Depuis
2015, les exemples montrant qu'il est possible d'induire facilement en erreur
un modèle appris par apprentissage automatique s'accumulent. Ces exemples
incluent l'ajout de bruit
indétectable \href{https://arxiv.org/abs/1412.6572}{à l'\oe{}il} ou
\href{https://nicholas.carlini.com/code/audio\_adversarial\_examples}{à
  l'oreille}, la \href{https://arxiv.org/abs/1710.08864}{modification d'un seul
  pixel} d'une image, ou
l'\href{https://towardsdatascience.com/poisoning-attacks-on-machine-learning-1ff247c254db}{empoisonnement}
d'un jeu de données, qui consiste à introduire au moment de l'apprentissage un
faible nombre d'exemples mal étiquetés ou ingénieusement calibrés pour induire
un comportement indésirable.

De même qu'en cryptographie où de nouveaux protocoles émergent pour faire face
à de nouvelles attaques de hackers, l'apprentissage automatique progresse aussi
pour répondre aux attaques
adversariales. \href{http://proceedings.mlr.press/v97/simon-gabriel19a.html}{De
  récents travaux} montrent même qu'en raison du fléau de la dimension, les
attaques adversariales sont inévitables en grande dimension.

\paragraph{Reproductibilité} La démarche scientifique repose sur la
reproductibilité des expériences. Se posent alors la question de la
disponibilité des données, qui peut être limitée pour des raisons de
confidentialité, et celle des \textbf{ressources informatiques} qui peuvent
être nécessaires à entraîner certains modèles. Reproduire des résultats
obtenuse en faisant tourner 800 processeurs graphiques (GPUs) pendant 3
semaines nécessite des ressources financières importantes (on rejoint ici des
questions de coût énergétique et écologique abordées dans la
section~\ref{sec:ecology}).

\paragraph{Responsabilité} Qui est responsable en cas de faillite d'un
système d'IA : l'IA est-elle responsable ? Ou bien la personne qui l'utilise ?
Ou encore celle qui l'a construite ? La question s'est par exemple posée
lorsqu'un véhicule autonome
\href{https://www.nextinpact.com/news/108432-cause-probable-accident-mortel-uber-tout-monde-en-prend-pour-son-grade.htm}{a
  fauché une piétonne} en mars 2018.


\section{Confidentialité des données}
Une grande partie des données utilisées en science des données sont des données
personnelles, c'est-à-dire que les individus qu'elles décrivent sont des
personnes. Nombre d'entre nous s'inquiètent de ce que les données qui nous
concernent, qu'elles soient médicales, de localisation géographique, ou
concernent notre activité numérique, soient utilisées à bon escient.

Les \href{https://risques-tracage.fr/}{discussions autour des applications de traçage de
contacts}
dans la lutte contre la propagation du coronavirus ont bien illustré cette
préoccupation.


En tant que \textit{data scientists}, comment nous assurer que nous ne
compromettons pas la confidentialité des personnes dont nous manipulons les
données ? Deux types de solutions techniques sont possibles.
\paragraph{Dé-identification algorithmique} Il s'agit de s'assurer que l'on ne
puisse pas remonter des données aux individus. Parmi ces techniques,
l'\textbf{anonymisation} consiste à supprimer suffisamment d'informations
identifiantes pour empêcher la réidentification. Ces informations sont dites
\textbf{directement identifiantes} s'il s'agit de caractéristiques personnelles
uniques (nom, numéro de sécurité sociale, numéro de téléphone, etc.) et
\textbf{indirectement identifiantes} si elles permettent d'identifier la
personne de manière unique quand elles sont croisées avec d'autres données
(code postal, date de naissance et lieu de travail pris ensemble peuvent être
indirectement identifiants).  Par contraste, la \textbf{confidentialité
  différentielle}, ou \textit{differential privacy} en anglais cherche plutôt à
garantir que les résultats d'une analyse sur une base de données soient presque
identiques qu'un échantillon soit présent ou non.

\paragraph{Sécurité des bases de données} Cet aspect inclut par exemple le
chiffrement homomorphique permettant d'obtenir les mêmes résultats sur données
chiffrées que non chiffrées, ne laissant ainsi aux \textit{data scientists} que
l'accès aux données chiffrées, des solutions de calcul distribué sécurisées, ou
encore du matériel cryptographique permettant d'exécuter du code sans que les
données ne soient visibles.

En France, la \href{https://www.cnil.fr/}{Commission Nationale de
  l'Informatique et des Libertés (CNIL)} encadre l'utilisation des données
personnelles, qui est notamment encadré par la loi du 14 mai 2018 transposant
le Règlement Général sur la Protection des Données (RGPD) de l'Union
Européenne.


\section{Enjeux écologiques}
\label{sec:ecology}
\href{https://infos.ademe.fr/magazine-avril-2022/faits-et-chiffres/numerique-quel-impact-environnemental/}{Selon
  l'ADEME}, le secteur du numérique est responsable de 2.5\% de l'empreinte carbone de la France, correspondant à 10\% de notre consommation électrique annuelle. Entraîner un réseau de neurones artificiels avec 213 millions de
paramètres peut générer \href{https://arxiv.org/abs/1906.02243}{autant
  d'émissions de CO2 que cinq voitures américaines} pendant toute leur
existence, fabrication comprise. Le
\href{https://mlco2.github.io/impact/}{ML Emissions Calculator}
est un des outils qui accompagnent la prise de conscience de l'impact
environnemental de la science des données. Ces enjeux deviennent d'autant plus importants que l'on développe de très grands modèles, notamment en traitement automatisé du langage ; deux exemples récents de travaux sur ces questions sont \href{https://arxiv.org/abs/2304.03271}{\textit{Making AI less thirsty: uncovering and addressing the secret water footprint of AI models}}, qui s'intéresse à la consommation en eau des modèles, et \href{https://arxiv.org/abs/2211.02001}{\textit{Estimating the Carbon Footprint of BLOOM}}, qui essaie d'estimer l'empreinte carbone d'un modèle de langage à 176 milliards de paramètres.

\begin{plusloin}
\item Des ouvrages entiers ont étés écrits sur la \textit{dataviz}, par exemple \href{https://clauswilke.com/dataviz/}{\textit{Fundamentals of Data Vizualization} de Claus O. Wilke}, le travail d'\href{https://www.edwardtufte.com/tufte/}{Edward Tufte}, \href{https://informationisbeautiful.net/}{\textit{Information is Beautiful} de David McCandless}, ou encore \href{https://github.com/rougier/scientific-visualization-book}{\textit{Scientific Visualization} de Nicolas Rougier}.
% \item \href{https://hippocrate.tech/}{Le Serment d'Hippocrate pour Data Scientist} de Data for Good.
\item La représentativité est une question qui revient dans de nombreux
  domaines de l'ingénierie. Les exemples sont nombreux, des
  \href{https://www.huffingtonpost.fr/2017/08/19/ce-distributeur-automatique-ne-distribue-pas-de-savon-aux-mains\_a\_23152387/}{distributeurs
    de savon qui ne détectent que les peaux claires} à tous les
  objets plutôt adaptés aux hommes recensés par Caroline Criado
  Perez dans
  \href{https://www.liberation.fr/france/2020/03/06/les-femmes-invisibles-dans-un-monde-cree-pour-les-hommes\_1780895}{\textit{Invisible
      Women}}.
  \item Un épisode de La Méthode Scientifique  intitulé \href{https://april.org/ethique-numerique-des-datas-sous-serment-emission-la-methode-scientifique}{\textit{Éthique numérique, des data sous
    serment}}.
  \item \href{https://fairmlbook.org/}{\textit{Fairness and Machine Learning}} de Solon
    Barocas, Moritz Hardt and Arvind Narayanan.
  \item À propos de \href{https://www.latribune.fr/supplement/ceux-qui-transforment-la-france/la-justice-predictive-nouvel-outil-pour-les-professionnels-du-droit-837752.html}{justice prédictive}, l'article \href{https://www.dalloz-actualite.fr/flash/justice-et-intelligence-artificielle-preparer-demain-episode-i}{Justice
        et intelligence artificielle : préparer demain} dans Dalloz Actualité.
  \item \href{https://hbr.org/2013/04/the-hidden-biases-in-big-data}{\textit{The Hidden Biases in Big Data}}, Kate Crawford, HBR, April 2013. 
  \item \href{https://salil.seas.harvard.edu/files/salil/files/differential_privacy_primer_nontechnical_audience.pdf}{\textit{Differential privacy: A primer for a non-technical audience}}, A. Wood et al., Vanderbilt Journal of \\Entertainment and Technology Law.
  \item Le \href{https://ethics-of-ai.mooc.fi/start}{cours d'éthique de l'IA de l'Université d'Helsinki}
\end{plusloin}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../sdd_2025_poly"
%%% End:

